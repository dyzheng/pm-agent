# DZ-010: 更新 toolchain 文档：InfiniBand/OFI 环境编译指南

## Objective

基于 #6195 (DZ-003) 的诊断结果，更新 ABACUS 编译文档，添加 InfiniBand 环境下使用 toolchain 编译的注意事项，说明 OpenMPI OFI/UCX 传输层的配置方法，以及常见错误的排查步骤。

## Reference Code

### Source Code (Toolchain 脚本)

**`/root/abacus-develop/toolchain/scripts/stage1/install_openmpi.sh`**
- Lines 33-37: OpenMPI 版本
  - 主版本: 5.0.8
  - 备选版本: 4.1.6
- Lines 94-106: configure 命令
  ```bash
  ./configure --prefix=${pkg_install_dir} --libdir="${pkg_install_dir}/lib" \
              --with-libevent=internal
  ```
  - 当前未指定 `--with-ofi` / `--with-ucx` / `--without-ofi` 等传输层选项
- Lines 116-120: MPI 编译器检测
- Lines 181-203: 环境变量导出 (MPIRUN, MPICC, MPICXX, MPIFC 等)

**`/root/abacus-develop/toolchain/scripts/common_vars.sh`**
- Line 31: `MPI_MODE=${MPI_MODE:-openmpi}` — 默认 OpenMPI
- 支持: openmpi, mpich, intelmpi

**`/root/abacus-develop/toolchain/scripts/stage3/install_elpa.sh`**
- Line 50: ELPA 依赖 MPI
- Lines 146-154: ELPA configure 使用 MPI 编译器

**`/root/abacus-develop/CMakeLists.txt`**
- Line 13: `option(ENABLE_MPI "Enable MPI" ON)`
- Line 326: `find_package(MPI REQUIRED)`

### Target Code (文档)

**`/root/abacus-develop/docs/advanced/install.md`** — 安装文档（需要更新）

## Implementation Guide

### Architecture Decisions

- 文档更新应覆盖三种常见场景：
  1. 无 InfiniBand 的普通集群（TCP 传输，无需特殊配置）
  2. 有 InfiniBand 但权限受限的环境（需要绕过 OFI/IB）
  3. 有 InfiniBand 且权限正常的 HPC 环境（推荐 UCX 传输）

### 文档内容大纲

```markdown
## InfiniBand 环境编译注意事项

### 常见错误

#### OFI Libfabric 错误
```
Failed to modify UD QP to INIT on mlx5_0: Operation not permitted
Open MPI failed an OFI Libfabric library call (fi_endpoint)
Error: Invalid argument (22)
```

**原因**: OpenMPI 默认启用 OFI (libfabric) 传输层，在 InfiniBand 环境中
会尝试使用 mlx5 provider。如果用户没有 InfiniBand 设备的访问权限，
会导致此错误。

**解决方案**:

方案 1: 运行时绕过 OFI (推荐，无需重新编译)
```bash
export OMPI_MCA_btl=tcp,self
export OMPI_MCA_mtl=^ofi
mpirun -np 4 abacus
```

方案 2: 编译 OpenMPI 时禁用 OFI
```bash
# 修改 toolchain 配置
export OPENMPI_OFI=no
bash install_openmpi.sh
```

方案 3: 使用 UCX 替代 OFI (HPC 环境推荐)
```bash
# 先安装 UCX
./configure --prefix=/path/to/ucx --with-mlx5-dv
make -j && make install
# 编译 OpenMPI 时指定 UCX
./configure --prefix=... --with-ucx=/path/to/ucx --without-ofi
```

### 验证 MPI 传输层
```bash
# 查看可用传输层
ompi_info --parsable --all | grep btl
ompi_info --parsable --all | grep mtl

# 测试 MPI 通信
mpirun -np 2 --mca btl tcp,self hostname
```
```

### Toolchain 脚本改进建议

在 `install_openmpi.sh` 中添加可选的传输层配置：

```bash
# 新增环境变量支持
OPENMPI_TRANSPORT=${OPENMPI_TRANSPORT:-auto}  # auto, tcp, ucx, ofi

case "${OPENMPI_TRANSPORT}" in
    tcp)
        TRANSPORT_FLAGS="--without-ofi --without-ucx"
        ;;
    ucx)
        TRANSPORT_FLAGS="--with-ucx=${UCX_PREFIX:-/usr} --without-ofi"
        ;;
    ofi)
        TRANSPORT_FLAGS="--with-ofi=${OFI_PREFIX:-/usr}"
        ;;
    auto|*)
        TRANSPORT_FLAGS=""
        ;;
esac

./configure --prefix=${pkg_install_dir} ${TRANSPORT_FLAGS} ...
```

### Critical Implementation Details

- OpenMPI 4.x 和 5.x 的传输层默认配置不同
  - 4.x: 默认 BTL (byte transfer layer) + OFI
  - 5.x: 默认 UCX (如果可用)
- `OMPI_MCA_btl=tcp,self` 是最简单的绕过方案，但会损失 RDMA 性能
- 对于 HPC 用户，推荐使用 UCX 而非完全禁用高性能传输
- 文档应明确说明这不是 ABACUS 的 bug，而是 MPI 环境配置问题

## TDD Test Plan

### Tests to Write FIRST

```bash
# 文档验证脚本 (非代码测试)

# 1. 验证文档中的命令可执行
ompi_info --parsable --all | grep -c btl  # 应 > 0

# 2. 验证 TCP fallback 可用
mpirun -np 2 --mca btl tcp,self hostname  # 应成功

# 3. 验证环境变量方式
export OMPI_MCA_btl=tcp,self
export OMPI_MCA_mtl=^ofi
mpirun -np 2 hostname  # 应成功，无 OFI 错误
```

## Acceptance Criteria

- [ ] 文档中包含 InfiniBand 环境编译的专门章节
- [ ] 包含常见 OFI/UCX 错误的排查步骤
- [ ] 至少一位用户确认文档有帮助
